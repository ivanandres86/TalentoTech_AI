{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhliWIjbj4sjjuMlgyYd47"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLtTffeJHZQf","executionInfo":{"status":"ok","timestamp":1730942938297,"user_tz":300,"elapsed":194,"user":{"displayName":"Ivan Andres Diaz","userId":"10340677963396566566"}},"outputId":"223b8e73-4830-4325-f408-a080c93dbf47"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'el': 5, 'buen': 6, 'dia': 7, 'como': 8, 'estas': 9, 'hoy': 10}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 4, 5, 2], [6, 7, 8, 9]]\n","rellena =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 4 5 2]\n"," [0 6 7 8 9]]\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","frase =[\n","     'Hola mundo',\n","     'Hola a todos',\n","     'Hola a todos el mundo',\n","     'Buen dia, como estas hoy',\n","]\n","\n","#Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words =10)\n","tokenizer.fit_on_texts(frase)\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","#generacion de secuencia tokenizadas\n","secuencias =tokenizer.texts_to_sequences(frase)\n","print('secuencias =',secuencias)\n","\n","#rellena las secuencias a una longitud uniforme\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n',relleno)"]},{"cell_type":"markdown","source":["# ejercicio oov_token='<OOV>',**texto en negrita**"],"metadata":{"id":"ErrbvikWPVzh"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","frase =[\n","     'Hola mundo',\n","     'Hola a todos',\n","     'Hola a todos el mundo',\n","     'Buen dia, como estas hoy',\n","]\n","\n","#Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words =10, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frase)\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","#generacion de secuencia tokenizadas\n","secuencias =tokenizer.texts_to_sequences(frase)\n","print('secuencias =',secuencias)\n","\n","#rellena las secuencias a una longitud uniforme\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJaYDB0bPtpJ","executionInfo":{"status":"ok","timestamp":1730943121894,"user_tz":300,"elapsed":210,"user":{"displayName":"Ivan Andres Diaz","userId":"10340677963396566566"}},"outputId":"b766a091-52b2-4c84-9f64-fa509a5401cd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 5, 6, 3], [7, 8, 9, 1, 1]]\n","rellena =\n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 5 6 3]\n"," [7 8 9 1 1]]\n"]}]},{"cell_type":"markdown","source":["**3.** Ahora vamos a probar los parámetros padding y truncating de la función pad_sequences en\n","Keras se utilizan para controlar el relleno y el truncamiento de las secuencias durante el preprocesamiento de datos."],"metadata":{"id":"JIXz8-uYQ7r_"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","frase =[\n","     'Hola mundo',\n","     'Hola a todos',\n","     'Hola a todos el mundo',\n","     'Buen dia, como estas hoy',\n","]\n","\n","#Genera el diccionario de tokens\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words =10, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frase)\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","#generacion de secuencia tokenizadas\n","secuencias =tokenizer.texts_to_sequences(frase)\n","print('secuencias =',secuencias)\n","\n","#rellena las secuencias a una longitud uniforme\n","relleno= keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('rellena =\\n',relleno)\n","\n","# Relleno al final\n","relleno_post = keras.preprocessing.sequence.pad_sequences(secuencias, padding='post')\n","\n","# Truncamiento al final\n","relleno_trunc_post = keras.preprocessing.sequence.pad_sequences(secuencias, truncating='post', maxlen=3)\n","\n","\n","print('rellena post =\\n',relleno_post)\n","print('rellena trunc post =\\n',relleno_trunc_post)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAcWx4PzRIZ_","executionInfo":{"status":"ok","timestamp":1730943898147,"user_tz":300,"elapsed":260,"user":{"displayName":"Ivan Andres Diaz","userId":"10340677963396566566"}},"outputId":"ddb29261-fb07-4cfe-eaf6-44022bf9c416"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 5, 6, 3], [7, 8, 9, 1, 1]]\n","rellena =\n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 5 6 3]\n"," [7 8 9 1 1]]\n","rellena post =\n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 5 6 3]\n"," [7 8 9 1 1]]\n","rellena trunc post =\n"," [[0 2 3]\n"," [2 4 5]\n"," [2 4 5]\n"," [7 8 9]]\n"]}]}]}